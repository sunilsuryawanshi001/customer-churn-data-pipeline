# Customer Churn Data Engineering & Analytics Pipeline

## üìå Overview

This project demonstrates an **end-to-end data engineering pipeline** built using **PostgreSQL, SQL, and Python**.  
The primary goal is to transform raw customer data into **clean, analytics-ready and ML-ready datasets** using layered SQL transformations.

A lightweight machine learning pipeline is included **only as a downstream consumer** to validate how curated data can be used for churn risk analysis.

> **Primary focus:** Data pipelines, SQL transformations, and data modeling  
> **Secondary focus:** Downstream ML consumption (not ML optimization)

---

## üéØ Problem Statement

Customer churn is a key business challenge where organizations want to identify customers who are likely to stop using a service.

From a **data engineering perspective**, the challenge is to:
- ingest customer data reliably
- clean and standardize it using SQL
- model it into reusable layers
- expose it for analytics and predictive use cases

---

## üß± High-Level Architecture

Raw Data Source
‚Üì
PostgreSQL (RAW schema)
‚Üì
PostgreSQL (CORE schema)
‚Üì
PostgreSQL (ML schema ‚Äì Feature Tables)
‚Üì
Python ML Pipeline (Downstream Consumer)


---

## üìÅ Project Structure

customer-churn-data-pipeline/
‚îÇ
‚îú‚îÄ‚îÄ .env
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ README.md
‚îÇ
‚îú‚îÄ‚îÄ data_pipeline/
‚îÇ ‚îî‚îÄ‚îÄ src/
‚îÇ ‚îî‚îÄ‚îÄ loaddata.py
‚îÇ
‚îú‚îÄ‚îÄ sql/
‚îÇ ‚îú‚îÄ‚îÄ raw/
‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ create_raw_tables.sql
‚îÇ ‚îÇ
‚îÇ ‚îú‚îÄ‚îÄ core/
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ create_core_tables.sql
‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ transform_raw_to_core.sql
‚îÇ ‚îÇ
‚îÇ ‚îú‚îÄ‚îÄ ml/
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ create_ml_customer_churn_features.sql
‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ populate_ml_customer_churn_features.sql
‚îÇ ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ validation/
‚îÇ ‚îî‚îÄ‚îÄ data_quality_checks.sql
‚îÇ
‚îú‚îÄ‚îÄ ml_pipeline/
‚îÇ ‚îú‚îÄ‚îÄ src/
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ data_loader.py
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ preprocess.py
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ train_and_predict.py
‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ risk_analysis.py
‚îÇ ‚îÇ
‚îÇ ‚îú‚îÄ‚îÄ models/
‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ churn_model.pkl
‚îÇ ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ run_pipeline.py
‚îÇ
‚îî‚îÄ‚îÄ docs/
‚îî‚îÄ‚îÄ architecture.md


---

## üß© Data Pipeline (Core Focus)

The **data pipeline** is responsible for preparing high-quality data using SQL-first transformations.

### Responsibilities
- Ingest raw customer data into PostgreSQL
- Apply deterministic SQL transformations
- Build analytics- and ML-ready tables
- Enforce data quality checks

### Data Layers

| Layer | Description |
|-----|-------------|
| RAW | Source data stored without modification |
| CORE | Cleaned and standardized customer entities |
| ML | Feature tables designed for predictive use |
| Validation | Data quality and consistency checks |

---

## ü§ñ ML Pipeline (Downstream Consumer)

The ML pipeline **consumes curated feature tables** produced by the data pipeline.

### ML Workflow
1. Load features from `ml.customer_churn_features`
2. Apply preprocessing (scaling, encoding)
3. Split data into training and testing sets
4. Train a baseline churn prediction model
5. Evaluate using ROC-AUC
6. Identify high-risk churn customers

> The ML pipeline does not perform data ingestion or SQL transformations.

---

## üìä Sample Model Output

- **ROC-AUC:** ~0.87
- High recall for churned customers
- Ranked churn probabilities for risk-based segmentation

Example:

| customer_id | churn_probability | risk_level |
|-------------|-------------------|------------|
| 1137        | 0.99              | HIGH       |
| 1672        | 0.99              | HIGH       |
| 1278        | 0.99              | HIGH       |

---

## ‚öôÔ∏è Technologies Used

### Core Technologies
- PostgreSQL
- SQL
- Python

### Python Libraries
- pandas
- psycopg2
- SQLAlchemy
- python-dotenv
- scikit-learn

---

## ‚ñ∂Ô∏è How to Run the Project

### 1Ô∏è‚É£ Configure environment variables

Create a `.env` file in the project root:

```env
DB_HOST=localhost
DB_PORT=5432
DB_NAME=your_database
DB_USER=your_user
DB_PASSWORD=your_password


### 2Ô∏è‚É£ Install dependencies

pip install -r requirements.txt


### 3Ô∏è‚É£ Run SQL pipeline

Execute SQL scripts in the following order:

sql/raw

sql/core

sql/ml

sql/validation


### 4Ô∏è‚É£ Run ML pipeline

cd ml_pipeline
python run_pipeline.py


üß† Key Design Principles

SQL-first data transformations

Layered schema design (RAW ‚Üí CORE ‚Üí ML)

Clear separation of data and ML pipelines

Deterministic feature engineering

Environment-based configuration management


üöÄ Future Enhancements

Workflow orchestration using Apache Airflow

BI dashboards for churn analytics

Prediction storage and monitoring

Cloud deployment (GCP / AWS)
